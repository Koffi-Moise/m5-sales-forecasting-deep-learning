# -*- coding: utf-8 -*-
"""M5-forcasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_VcHTnkR7LM4nsNnkNGakV4kwEPz3j6

# Devoir Forcasting

## Import des modules
"""

import numpy as np
import pandas as pd


import os
for dirname, _, filenames in os.walk('C:\\Users\\Utilisateur\\Desktop\\BE\\Fichiers\\Les_projets\\Data_Analyst\\Data\\m5'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""## Chargement et aperçu des données"""

INPUT_DIR='C:\\Users\\Utilisateur\\Desktop\\BE\\Fichiers\\Les_projets\\Data_Analyst\\Data\\m5\\'
cal_data=pd.read_csv(INPUT_DIR+'calendar.csv')
salestv_data=pd.read_csv(INPUT_DIR+'sales_train_validation.csv')
ss_data=pd.read_csv(INPUT_DIR+'sample_submission.csv')
sellp_data=pd.read_csv(INPUT_DIR+'sell_prices.csv')

timesteps= 14
startDay= 0

salestv_data.shape

salestv_data.head()

cal_data.head()

salestv_data.describe()

"""## Transformations des dataset"""

def downcast_dtypes(df):
    float_cols = [c for c in df if df[c].dtype == 'float64']
    int_cols = [c for c in df if df[c].dtype in ['int32', 'int64']]
    df[float_cols] = df[float_cols].astype(np.float32)
    df[int_cols] = df[int_cols].astype(np.int16)
    return df

salestv_data_downcast = downcast_dtypes(salestv_data)

print(salestv_data_downcast.info(5))

salestv_data_downcast = salestv_data_downcast.T

salestv_data_downcast.shape
salestv_data_downcast.head(8)

#suppression des 6 premières lignes (id, item_id, dept_id, cat_id, store_id, state_id)
salestv_data_downcast = salestv_data_downcast[6+startDay:]
salestv_data_downcast.head(8)

# création d'un dataframe avc des zéros pour 1969 jours dans le calendrier - Pour stocker la connaissance si un événement existe le jour suivant
daysBeforeEvent = pd.DataFrame(np.zeros((1969,1)))

# 1 est assigné au jour avant event_name_1
for x, y in cal_data.iterrows():
    if x!=0 and (pd.isnull(cal_data['event_name_1'][x]) == False):
        daysBeforeEvent[0][x-1]=1

# daysBeforeEvent is used as a feature and also to train and test the validations
daysBeforeEventTest = daysBeforeEvent[1913:1941]
daysBeforeEventTrain = daysBeforeEvent[startDay:1913]
print(daysBeforeEventTest.shape)
print(daysBeforeEventTest[:5])

daysBeforeEventTrain.columns = ['oneDayBeforeEvent']
daysBeforeEventTrain.index = salestv_data_downcast.index

final_data = pd.concat([salestv_data_downcast, daysBeforeEventTrain], axis=1)
final_data.head(10)

"""## Sélection de 10 produits"""

# Sélection de 10 colonnes aléatoires parmi les colonnes nommés de 0 à 30489 en ajoutant la dernière colonne

product_columns = final_data.columns[:-1]

np.random.seed(42)

selected_products = np.random.choice(product_columns, size=10, replace=False)

columns_to_keep = list(selected_products) + ['oneDayBeforeEvent']


df_sample = final_data[columns_to_keep]

print("✅ 10 produits sélectionnés aléatoirement :")
print(selected_products)
print("\nAperçu du DataFrame résultant :")
df_sample.head(10)

print(df_sample.dtypes)

df_sample[selected_products]= df_sample[selected_products].astype(int)

df_sample.columns = df_sample.columns.astype(str)

from sklearn.preprocessing import MinMaxScaler

sc = MinMaxScaler(feature_range=(0,1))
final_data_scaled = sc.fit_transform(df_sample)

print(final_data_scaled)

final_data_scaled.shape

# --- Initialisation des listes ---
X_train = []
y_train = []

# --- Génération des séquences ---
for i in range(timesteps, final_data_scaled.shape[0] - startDay):
    X_train.append(final_data_scaled[i - timesteps:i])
     # Valeur cible = les 10 produits au jour i
    y_train.append(final_data_scaled[i, :-1])

X_train = np.array(X_train)
y_train = np.array(y_train)

print("✅ X_train shape:", X_train.shape)
print("✅ y_train shape:", y_train.shape)

"""## MLP model - keras implementation"""

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten

mlp = Sequential([
    Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(y_train.shape[1])
])

mlp.compile(optimizer='adam', loss='mean_squared_error')

mlp.fit(X_train, y_train, epochs=32, batch_size=44)

"""## RNN Simple model - keras implementation"""

from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Dropout

rnn = Sequential([
    SimpleRNN(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.2),
    SimpleRNN(64),
    Dropout(0.2),
    Dense(y_train.shape[1])
])

rnn.compile(optimizer='adam', loss='mean_squared_error')

rnn.fit(X_train, y_train, epochs=32, batch_size=44)

"""## GRU model - keras implementation"""

from keras.models import Sequential
from keras.layers import GRU, Dense, Dropout

gru_model = Sequential([
    GRU(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.2),
    GRU(64),
    Dropout(0.2),
    Dense(y_train.shape[1])  # 10 sorties produits
])

gru_model.compile(optimizer='adam', loss='mean_squared_error')

gru_model.fit(X_train, y_train, epochs=32, batch_size=44)

"""## Evaluation des modèles"""

inputs = df_sample[-timesteps:]
inputs = sc.transform(inputs)
print(inputs)

X_test = []
X_test.append(inputs[0:timesteps])
X_test = np.array(X_test)
print(X_test.shape)
print(X_test[0, 1:14].shape)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# --- Fonction pour calculer toutes les métriques ---
def safe_mape(y_true, y_pred):
    # on remplace les zéros par un petit nombre epsilon
    epsilon = 1e-8
    y_true_safe = np.where(y_true == 0, epsilon, y_true)
    return np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100


def evaluate_model_safe(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = safe_mape(y_true, y_pred)

    # filtrer les NaN ou inf avant R2
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    r2 = r2_score(y_true[mask], y_pred[mask])
    return mae, mse, rmse, mape, r2



# Exemples pour les 3 modèles
models = {'MLP': mlp, 'SimpleRNN': rnn, 'GRU': gru_model}

results = {}

for name, model in models.items():
    print(f"\nEvaluating {name} ...")



    # Exemple pour y_true


    # --- Prédiction sur X_test ---
    y_pred_scaled = model.predict(X_test)

    y_pred = y_pred_scaled

    y_pred_full = np.hstack([y_pred, np.zeros((y_pred.shape[0], 1))])  # ajouter colonne factice
    y_pred_inverse = sc.inverse_transform(y_pred_full)[:, :-1]          # récupérer les 10 produits

    # --- Valeurs réelles correspondantes ---
    y_true = y_train[-y_pred.shape[0]:]

    y_true_full = np.hstack([y_true, np.zeros((y_true.shape[0], 1))])
    y_true_inverse = sc.inverse_transform(y_true_full)[:, :-1]

    # --- Calcul des métriques ---
    mae, mse, rmse, mape, r2 = evaluate_model_safe(y_true_inverse, y_pred_inverse)

    # --- Stockage des résultats ---
    results[name] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}

# --- Affichage résumé ---
import pandas as pd
results_df = pd.DataFrame(results).T
print("\n✅ Résultats des modèles :")
print(results_df)

"""## Conclusion

SimpleRNN est actuellement le meilleur modèle pour tes données.

GRU est proche, mais légèrement moins performant que SimpleRNN ici.

MLP est bien moins performant car il ne capture pas la dynamique temporelle.
"""